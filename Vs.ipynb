{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='pipeline.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ====================== Path Configuration ==========================\n",
    "BASE_DIR = r\"C:\\Users\\mitta\\OneDrive\\Desktop\\pbl enhanced\\chest_xray\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"test\")\n",
    "PREPROCESSED_DIR = r\"C:\\Users\\mitta\\OneDrive\\Desktop\\pbl enhanced\\preprocessed\"\n",
    "ENHANCED_DIR = r\"C:\\Users\\mitta\\OneDrive\\Desktop\\pbl enhanced\\enhanced_image\"\n",
    "\n",
    "os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(ENHANCED_DIR, exist_ok=True)\n",
    "\n",
    "# ====================== Data Analysis Functions =====================\n",
    "def count_images(directory):\n",
    "    \"\"\"Count number of images in NORMAL and PNEUMONIA categories with error handling.\"\"\"\n",
    "    categories = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "    counts = {}\n",
    "    for cat in categories:\n",
    "        try:\n",
    "            cat_path = os.path.join(directory, cat)\n",
    "            if os.path.exists(cat_path):\n",
    "                counts[cat] = len([f for f in os.listdir(cat_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            else:\n",
    "                counts[cat] = 0\n",
    "                logging.warning(f\"Directory not found: {cat_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error counting images in {cat_path}: {str(e)}\")\n",
    "            counts[cat] = 0\n",
    "    return counts\n",
    "\n",
    "def plot_class_distributions():\n",
    "    \"\"\"Plot class distributions across train, val, and test sets.\"\"\"\n",
    "    sets = {'Train': TRAIN_DIR, 'Validation': VAL_DIR, 'Test': TEST_DIR}\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for i, (name, path) in enumerate(sets.items()):\n",
    "        counts = count_images(path)\n",
    "        if counts:\n",
    "            sns.barplot(x=list(counts.keys()), y=list(counts.values()), ax=ax[i])\n",
    "            ax[i].set_title(f\"{name} Set (Total: {sum(counts.values())})\")\n",
    "            ax[i].set_ylabel(\"Number of Images\")\n",
    "            \n",
    "            # Add percentage annotations\n",
    "            total = sum(counts.values())\n",
    "            for j, v in enumerate(counts.values()):\n",
    "                ax[i].text(j, v + 5, f\"{v/total:.1%}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distributions.png')\n",
    "    plt.show()\n",
    "\n",
    "# ====================== Image Preprocessing =========================\n",
    "def get_transform(augment=False):\n",
    "    \"\"\"Return preprocessing transform with optional augmentation.\"\"\"\n",
    "    base_transform = [\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "    ]\n",
    "    \n",
    "    if augment:\n",
    "        base_transform.insert(2, transforms.RandomHorizontalFlip())\n",
    "        base_transform.insert(2, transforms.RandomRotation(10))\n",
    "    \n",
    "    return transforms.Compose(base_transform)\n",
    "\n",
    "def preprocess_and_save_images(input_dir, output_dir):\n",
    "    \"\"\"Preprocess and save each image to output_dir with error handling.\"\"\"\n",
    "    transform = get_transform()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for category in [\"NORMAL\", \"PNEUMONIA\"]:\n",
    "        category_input = os.path.join(input_dir, category)\n",
    "        category_output = os.path.join(output_dir, category)\n",
    "        os.makedirs(category_output, exist_ok=True)\n",
    "        \n",
    "        # Get image list with progress bar\n",
    "        image_files = [f for f in os.listdir(category_input) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for img_name in tqdm(image_files, desc=f\"Processing {category}\"):\n",
    "            try:\n",
    "                img_path = os.path.join(category_input, img_name)\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                # Skip corrupted images\n",
    "                if img is None:\n",
    "                    logging.warning(f\"Failed to load image: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                tensor_img = transform(img)\n",
    "                img_np = tensor_img.squeeze().numpy() * 255\n",
    "                img_np = img_np.astype(np.uint8)\n",
    "                \n",
    "                save_path = os.path.join(category_output, img_name)\n",
    "                cv2.imwrite(save_path, img_np)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {img_path}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# ====================== Image Enhancement ===========================\n",
    "def apply_clahe(image_np):\n",
    "    \"\"\"Apply CLAHE for local contrast enhancement with error handling.\"\"\"\n",
    "    try:\n",
    "        if len(image_np.shape) == 3:\n",
    "            image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        return clahe.apply(image_np)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"CLAHE enhancement failed: {str(e)}\")\n",
    "        return image_np\n",
    "\n",
    "def enhance_images(input_dir, output_dir):\n",
    "    \"\"\"Enhance each image using CLAHE and save it with error handling.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for category in [\"NORMAL\", \"PNEUMONIA\"]:\n",
    "        input_path = os.path.join(input_dir, category)\n",
    "        output_path = os.path.join(output_dir, category)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        image_files = [f for f in os.listdir(input_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for img_name in tqdm(image_files, desc=f\"Enhancing {category}\"):\n",
    "            try:\n",
    "                img_path = os.path.join(input_path, img_name)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if img is None:\n",
    "                    logging.warning(f\"Failed to load image for enhancement: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                enhanced = apply_clahe(img)\n",
    "                save_path = os.path.join(output_path, img_name)\n",
    "                cv2.imwrite(save_path, enhanced)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error enhancing {img_path}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# ====================== Dataset Class ===============================\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset class for chest X-ray images with caching.\"\"\"\n",
    "    def __init__(self, directory, transform=None, cache=True):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self.cached_images = {}\n",
    "        \n",
    "        # Map categories to numerical labels\n",
    "        self.class_to_idx = {\"NORMAL\": 0, \"PNEUMONIA\": 1}\n",
    "        \n",
    "        for category in self.class_to_idx.keys():\n",
    "            cat_dir = os.path.join(directory, category)\n",
    "            if not os.path.exists(cat_dir):\n",
    "                logging.warning(f\"Directory not found: {cat_dir}\")\n",
    "                continue\n",
    "                \n",
    "            for img_name in os.listdir(cat_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(cat_dir, img_name))\n",
    "                    self.labels.append(self.class_to_idx[category])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cached_images and self.cache:\n",
    "            return self.cached_images[idx]\n",
    "        \n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            \n",
    "            if self.cache:\n",
    "                self.cached_images[idx] = (img, label)\n",
    "                \n",
    "            return img, label\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            # Return a dummy image if loading fails\n",
    "            dummy_img = torch.zeros((1, 224, 224), dtype=torch.float32)\n",
    "            return dummy_img, torch.tensor(0, dtype=torch.float32)\n",
    "\n",
    "# ====================== Loss Function ===============================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Enhanced Focal loss for handling class imbalance with auto-alpha.\"\"\"\n",
    "    def __init__(self, gamma=2.0, reduction='mean', alpha=None, dataset=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        # Calculate alpha automatically if dataset is provided\n",
    "        if alpha is None and dataset is not None:\n",
    "            targets = [label for _, label in dataset]\n",
    "            class_counts = torch.bincount(torch.stack(targets).long())\n",
    "            self.alpha = 1 - class_counts / class_counts.sum()\n",
    "        else:\n",
    "            self.alpha = alpha if alpha is not None else torch.tensor([0.25, 0.75])\n",
    "            \n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        \n",
    "        # Expand alpha to match batch size\n",
    "        alpha_t = self.alpha[0] * (1 - targets) + self.alpha[1] * targets\n",
    "        \n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# ====================== K-Fold Cross Validation =====================\n",
    "def prepare_kfold_data(n_splits=5):\n",
    "    \"\"\"Prepare k-fold cross validation datasets.\"\"\"\n",
    "    full_dataset = ChestXrayDataset(TRAIN_DIR, transform=get_transform())\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    folds = []\n",
    "    for train_idx, val_idx in kfold.split(full_dataset):\n",
    "        train_subset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "        folds.append((train_subset, val_subset))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# ====================== Visualization Utilities =====================\n",
    "def visualize_sample_images(directory=TRAIN_DIR, n_samples=3):\n",
    "    \"\"\"Visualize sample images from each class with preprocessing comparison.\"\"\"\n",
    "    fig, axes = plt.subplots(2, n_samples * 2, figsize=(20, 8))\n",
    "    \n",
    "    for i, category in enumerate([\"NORMAL\", \"PNEUMONIA\"]):\n",
    "        cat_dir = os.path.join(directory, category)\n",
    "        sample_files = [f for f in os.listdir(cat_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:n_samples]\n",
    "        \n",
    "        for j, img_name in enumerate(sample_files):\n",
    "            try:\n",
    "                # Original image\n",
    "                img_path = os.path.join(cat_dir, img_name)\n",
    "                orig_img = Image.open(img_path).convert(\"RGB\")\n",
    "                axes[i, j].imshow(orig_img)\n",
    "                axes[i, j].set_title(f\"Original {category}\")\n",
    "                axes[i, j].axis('off')\n",
    "                \n",
    "                # Preprocessed image\n",
    "                transform = get_transform()\n",
    "                proc_img = transform(orig_img).squeeze().numpy()\n",
    "                axes[i, j + n_samples].imshow(proc_img, cmap='gray')\n",
    "                axes[i, j + n_samples].set_title(f\"Processed {category}\")\n",
    "                axes[i, j + n_samples].axis('off')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error visualizing {img_path}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_images.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pixel_intensity(image_path):\n",
    "    \"\"\"Enhanced pixel intensity plot with preprocessing comparison.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "        transform = get_transform()\n",
    "        tensor_img = transform(img.convert(\"RGB\"))\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Original image histogram\n",
    "        img_np = np.array(img)\n",
    "        ax1.hist(img_np.ravel(), bins=50, color='blue', alpha=0.7)\n",
    "        ax1.set_title(\"Original Pixel Intensity\")\n",
    "        ax1.set_xlabel(\"Pixel Value\")\n",
    "        ax1.set_ylabel(\"Frequency\")\n",
    "        \n",
    "        # Processed image histogram\n",
    "        proc_np = tensor_img.squeeze().numpy()\n",
    "        ax2.hist(proc_np.ravel(), bins=50, color='green', alpha=0.7)\n",
    "        ax2.set_title(\"Processed Pixel Intensity\")\n",
    "        ax2.set_xlabel(\"Normalized Pixel Value\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pixel_intensity_comparison.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error plotting pixel intensity for {image_path}: {str(e)}\")\n",
    "\n",
    "# ====================== Main Workflow ===============================\n",
    "def main():\n",
    "    try:\n",
    "        print(\"üîç Analyzing class distributions...\")\n",
    "        plot_class_distributions()\n",
    "\n",
    "        print(\"\\nüßº Preprocessing images...\")\n",
    "        preprocess_and_save_images(TRAIN_DIR, os.path.join(PREPROCESSED_DIR, \"train\"))\n",
    "        preprocess_and_save_images(VAL_DIR, os.path.join(PREPROCESSED_DIR, \"val\"))\n",
    "        preprocess_and_save_images(TEST_DIR, os.path.join(PREPROCESSED_DIR, \"test\"))\n",
    "\n",
    "        print(\"\\n‚öôÔ∏è Enhancing images...\")\n",
    "        enhance_images(os.path.join(PREPROCESSED_DIR, \"train\"), os.path.join(ENHANCED_DIR, \"train\"))\n",
    "        enhance_images(os.path.join(PREPROCESSED_DIR, \"val\"), os.path.join(ENHANCED_DIR, \"val\"))\n",
    "        enhance_images(os.path.join(PREPROCESSED_DIR, \"test\"), os.path.join(ENHANCED_DIR, \"test\"))\n",
    "\n",
    "        print(\"\\nüñºÔ∏è Visualizing sample images...\")\n",
    "        visualize_sample_images()\n",
    "\n",
    "        print(\"\\nüìä Plotting pixel intensity of a sample image...\")\n",
    "        sample_path = os.path.join(TRAIN_DIR, \"NORMAL\", os.listdir(os.path.join(TRAIN_DIR, \"NORMAL\"))[0])\n",
    "        plot_pixel_intensity(sample_path)\n",
    "\n",
    "        print(\"\\n‚úÖ Pipeline complete. Logs saved to pipeline.log\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Pipeline failed: {str(e)}\", exc_info=True)\n",
    "        print(f\"‚ùå Pipeline failed. Check logs for details: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb188be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Layer, Average\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3da069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '69'\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "tf.random.set_seed(69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b89eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4173 images belonging to 2 classes.\n",
      "Found 2 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\New folder\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 780ms/step - accuracy: 0.6736 - loss: 0.7505 - val_accuracy: 0.5000 - val_loss: 5.8707 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 550ms/step - accuracy: 0.8199 - loss: 0.4529 - val_accuracy: 0.5000 - val_loss: 1.6110 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 676ms/step - accuracy: 0.8404 - loss: 0.4176 - val_accuracy: 0.5000 - val_loss: 1.6120 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 731ms/step - accuracy: 0.8640 - loss: 0.3904 - val_accuracy: 1.0000 - val_loss: 0.1925 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 728ms/step - accuracy: 0.8672 - loss: 0.3834 - val_accuracy: 0.5000 - val_loss: 1.0400 - learning_rate: 1.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 796ms/step - accuracy: 0.8970 - loss: 0.3357 - val_accuracy: 1.0000 - val_loss: 0.3033 - learning_rate: 1.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 792ms/step - accuracy: 0.8925 - loss: 0.3293 - val_accuracy: 1.0000 - val_loss: 0.0367 - learning_rate: 2.0000e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 739ms/step - accuracy: 0.8895 - loss: 0.3232 - val_accuracy: 0.5000 - val_loss: 0.4702 - learning_rate: 2.0000e-05\n",
      "Epoch 9/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 376ms/step - accuracy: 0.8942 - loss: 0.3162 - val_accuracy: 0.5000 - val_loss: 0.5749 - learning_rate: 2.0000e-05\n",
      "Epoch 10/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 389ms/step - accuracy: 0.8905 - loss: 0.3259 - val_accuracy: 0.5000 - val_loss: 0.5016 - learning_rate: 4.0000e-06\n",
      "Epoch 11/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 367ms/step - accuracy: 0.8914 - loss: 0.3245 - val_accuracy: 0.5000 - val_loss: 0.6360 - learning_rate: 4.0000e-06\n",
      "Epoch 1/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 937ms/step - accuracy: 0.6326 - loss: 0.6668 - val_accuracy: 0.5000 - val_loss: 1.9376 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 818ms/step - accuracy: 0.7146 - loss: 0.6245 - val_accuracy: 0.5000 - val_loss: 1.0681 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 834ms/step - accuracy: 0.7422 - loss: 0.6040 - val_accuracy: 0.5000 - val_loss: 0.9882 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.7919 - loss: 0.5406 - val_accuracy: 0.5000 - val_loss: 0.9047 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 668ms/step - accuracy: 0.7897 - loss: 0.5284 - val_accuracy: 0.5000 - val_loss: 1.0897 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 691ms/step - accuracy: 0.8086 - loss: 0.5057 - val_accuracy: 0.5000 - val_loss: 1.0675 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 705ms/step - accuracy: 0.8192 - loss: 0.4694 - val_accuracy: 0.5000 - val_loss: 1.1033 - learning_rate: 2.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 762ms/step - accuracy: 0.8415 - loss: 0.4622 - val_accuracy: 0.5000 - val_loss: 1.0815 - learning_rate: 2.0000e-05\n",
      "Epoch 1/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 415ms/step - accuracy: 0.4541 - loss: 0.8112 - val_accuracy: 0.5000 - val_loss: 1.1681 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 395ms/step - accuracy: 0.5266 - loss: 0.7575 - val_accuracy: 0.5000 - val_loss: 0.7172 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 406ms/step - accuracy: 0.5678 - loss: 0.7330 - val_accuracy: 0.5000 - val_loss: 0.6939 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 410ms/step - accuracy: 0.6087 - loss: 0.7209 - val_accuracy: 0.5000 - val_loss: 0.6946 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 405ms/step - accuracy: 0.6402 - loss: 0.7130 - val_accuracy: 0.5000 - val_loss: 0.6992 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 387ms/step - accuracy: 0.6499 - loss: 0.6946 - val_accuracy: 0.5000 - val_loss: 0.7000 - learning_rate: 2.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 370ms/step - accuracy: 0.6679 - loss: 0.6834 - val_accuracy: 0.5000 - val_loss: 0.7014 - learning_rate: 2.0000e-05\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 224ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 653ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 313ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.84      0.79      0.81       234\n",
      "   Pneumonia       0.88      0.91      0.89       390\n",
      "\n",
      "    accuracy                           0.87       624\n",
      "   macro avg       0.86      0.85      0.85       624\n",
      "weighted avg       0.86      0.87      0.86       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Custom Fuzzy Layer\n",
    "class FuzzyLayer(Layer):\n",
    "    def __init__(self, units=1, **kwargs):\n",
    "        super(FuzzyLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(shape=(self.units, input_shape[-1]), initializer='uniform', trainable=True)\n",
    "        self.sigmas = self.add_weight(shape=(self.units, input_shape[-1]), initializer='ones', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_exp = tf.expand_dims(inputs, 1)\n",
    "        centers_exp = tf.expand_dims(self.centers, 0)\n",
    "        sigmas_exp = tf.expand_dims(self.sigmas, 0)\n",
    "        fuzzy_output = tf.exp(-tf.square(inputs_exp - centers_exp) / (2 * tf.square(sigmas_exp)))\n",
    "        return tf.reduce_sum(fuzzy_output, axis=-1)\n",
    "\n",
    "# ‚úÖ Data Pipeline\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    shear_range=0.15,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Updated paths for enhanced images\n",
    "enhanced_data_path = r'C:\\Users\\mitta\\OneDrive\\Desktop\\pbl enhanced\\enhanced_image'\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    os.path.join(enhanced_data_path, 'train'),  # Path to enhanced train directory\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    subset='training'\n",
    ")\n",
    "val_data = train_gen.flow_from_directory(\n",
    "    os.path.join(enhanced_data_path, 'val'),  # Path to enhanced train directory\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_data = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    os.path.join(enhanced_data_path, 'test'),  # Path to enhanced test directory\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ‚úÖ Base Model Wrapper with FuzzyNet Head\n",
    "def create_fuzzy_model(base_model_class):\n",
    "    base = base_model_class(input_shape=(img_size, img_size, 3), include_top=False, weights='imagenet')\n",
    "    base.trainable = False\n",
    "\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = FuzzyLayer(units=16)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs=base.input, outputs=x)\n",
    "\n",
    "# ‚úÖ Create All Three Models\n",
    "model_mv2 = create_fuzzy_model(MobileNetV2)\n",
    "model_resnet = create_fuzzy_model(ResNet50)\n",
    "model_effnet = create_fuzzy_model(EfficientNetB0)\n",
    "\n",
    "# ‚úÖ Compile Them\n",
    "for model in [model_mv2, model_resnet, model_effnet]:\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# ‚úÖ Train Each Model\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2)\n",
    "]\n",
    "\n",
    "model_mv2.fit(train_data, validation_data=val_data, epochs=15, callbacks=callbacks)\n",
    "model_resnet.fit(train_data, validation_data=val_data, epochs=20, callbacks=callbacks)\n",
    "model_effnet.fit(train_data, validation_data=val_data, epochs=20, callbacks=callbacks)\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Prediction\n",
    "y_probs_mv2 = model_mv2.predict(test_data)\n",
    "y_probs_res = model_resnet.predict(test_data)\n",
    "y_probs_eff = model_effnet.predict(test_data)\n",
    "\n",
    "ensemble_probs = (3 * y_probs_mv2 + 0.5 * y_probs_res + 1* y_probs_eff) / (3+0.5+1)\n",
    "y_pred = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "# ‚úÖ Evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = test_data.classes\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Pneumonia'], yticklabels=['Normal', 'Pneumonia'])\n",
    "plt.title(\"Confusion Matrix - Ensemble\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a22a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class FuzzyLayer(Layer):\n",
    "    def __init__(self, units=1, **kwargs):\n",
    "        super(FuzzyLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(shape=(self.units, input_shape[-1]), \n",
    "                                     initializer='uniform', \n",
    "                                     trainable=True)\n",
    "        self.sigmas = self.add_weight(shape=(self.units, input_shape[-1]), \n",
    "                                    initializer='ones', \n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_exp = tf.expand_dims(inputs, 1)\n",
    "        centers_exp = tf.expand_dims(self.centers, 0)\n",
    "        sigmas_exp = tf.expand_dims(self.sigmas, 0)\n",
    "        fuzzy_output = tf.exp(-tf.square(inputs_exp - centers_exp) / (2 * tf.square(sigmas_exp)))\n",
    "        return tf.reduce_sum(fuzzy_output, axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(FuzzyLayer, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c87f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (use .keras extension recommended)\n",
    "model_mv2.save('fuzzy_model_mv2.keras')  # or .h5 format\n",
    "model_resnet.save('fuzzy_model_resnet.keras')\n",
    "model_effnet.save('fuzzy_model_effnet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a45cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\New folder\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To load the models later\n",
    "loaded_mv2 = tf.keras.models.load_model('fuzzy_model_mv2.keras', \n",
    "                                      custom_objects={'FuzzyLayer': FuzzyLayer})\n",
    "loaded_resnet = tf.keras.models.load_model('fuzzy_model_resnet.keras', \n",
    "                                        custom_objects={'FuzzyLayer': FuzzyLayer})\n",
    "loaded_effnet = tf.keras.models.load_model('fuzzy_model_effnet.keras', \n",
    "                                        custom_objects={'FuzzyLayer': FuzzyLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0095378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import joblib\n",
    "\n",
    "# Define the path to your enhanced images\n",
    "ENHANCED_IMAGE_PATH = r'C:\\Users\\mitta\\OneDrive\\Desktop\\pbl enhanced\\enhanced_image'\n",
    "def get_img_array(img_path, size):\n",
    "    \"\"\"Load and preprocess image\"\"\"\n",
    "    img = image.load_img(img_path, target_size=size)\n",
    "    array = image.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array / 255.\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"Generate Grad-CAM heatmap\"\"\"\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_output = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_output, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def display_gradcam(img_path, model, last_conv_layer_name, img_size=(224, 224)):\n",
    "    \"\"\"Display Grad-CAM only if pneumonia is predicted\"\"\"\n",
    "    img_array = get_img_array(img_path, img_size)\n",
    "    \n",
    "    # Make prediction first\n",
    "    pred = model.predict(img_array)[0][0]\n",
    "    confidence = pred if pred > 0.5 else (1 - pred)\n",
    "    \n",
    "    # Only proceed if pneumonia is predicted with confidence > 50%\n",
    "    if pred > 0.5:\n",
    "        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "        superimposed_img = cv2.addWeighted(img, 0.6, heatmap_colored, 0.4, 0)\n",
    "\n",
    "        # Show the image with label\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Pneumonia Detected ({pred*100:.2f}% confidence)\", fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Normal case detected ({(1-pred)*100:.2f}% confidence). No visualization shown.\")\n",
    "\n",
    "# Example usage:\n",
    "# Load your trained model (replace with your actual model loading code)\n",
    "# model = tf.keras.models.load_model('your_model_path.h5')\n",
    "\n",
    "# Define the last convolutional layer name for each model\n",
    "last_conv_layers = {\n",
    "    'MobileNetV2': 'out_relu',  # Typical last conv layer for MobileNetV2\n",
    "    'ResNet50': 'conv5_block3_out',  # For ResNet50\n",
    "    'EfficientNetB0': 'top_activation'  # For EfficientNetB0\n",
    "}\n",
    "\n",
    "# Choose which model to visualize (change as needed)\n",
    "model_name = 'MobileNetV2'  # or 'ResNet50' or 'EfficientNetB0'\n",
    "last_conv_layer_name = last_conv_layers[model_name]\n",
    "\n",
    "# Path to a test image (replace with your actual image path)\n",
    "test_image_path = r\"C:\\Users\\mitta\\OneDrive\\Desktop\\pbl enhanced\\enhanced_image\\test\\PNEUMONIA\\person1_virus_6.jpeg\"\n",
    "\n",
    "# Run visualization (only shows output if pneumonia is predicted)\n",
    "display_gradcam(test_image_path, model, last_conv_layer_name)\n",
    "# display_gradcam(test_image_path, model_resnet, last_conv_layer_name)\n",
    "# display_gradcam(test_image_path, model_effnet, last_conv_layer_name)# Using model_mv2 from previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efe00dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\New folder\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 183ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 582ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 235ms/step\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "enhanced_data_path = r'C:\\Users\\mitta\\OneDrive\\Desktop\\pbl enhanced\\enhanced_image'\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "test_data = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    os.path.join(enhanced_data_path, 'test'),  # Path to enhanced test directory\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Prediction\n",
    "y_probs_mv2 = loaded_mv2.predict(test_data)\n",
    "y_probs_res = loaded_resnet.predict(test_data)\n",
    "y_probs_eff = loaded_effnet.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c774e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
